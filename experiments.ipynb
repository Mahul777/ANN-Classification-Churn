{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15e5f8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\python311\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\python311\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\python311\\lib\\site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sahus\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\python311\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python311\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python311\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sahus\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b94384cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (10000, 12)\n",
      "y shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# ✅ 3. Import necessary libraries\n",
    "# data preprocessing means that we will be dealing with data in a way that is suitable for machine learning algorithms\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Import preprocessing libraries which is used for data preprocessing  \n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder \n",
    "# pickle is a library that is used to save and load data\n",
    "import pickle\n",
    "\n",
    "\n",
    "# ✅ 4. Load the dataset\n",
    "# load the dataset\n",
    "data = pd.read_csv(\"Churn_Modelling.csv\")\n",
    "data.head()\n",
    "# | RowNumber | CustomerId | Surname  | CreditScore | Geography | Gender | Age | Tenure | Balance   | NumOfProducts | HasCrCard | IsActiveMember | EstimatedSalary | Exited |\n",
    "# | --------- | ---------- | -------- | ----------- | --------- | ------ | --- | ------ | --------- | ------------- | --------- | -------------- | --------------- | ------ |\n",
    "# | 1         | 15634602   | Hargrave | 619         | France    | Female | 42  | 2      | 0.00      | 1             | 1         | 1              | 101348.88       | 1      |\n",
    "# | 2         | 15647311   | Hill     | 608         | Spain     | Female | 41  | 1      | 83807.86  | 1             | 0         | 1              | 112542.58       | 0      |\n",
    "# | 3         | 15619304   | Onio     | 502         | France    | Female | 42  | 8      | 159660.80 | 3             | 1         | 0              | 113931.57       | 1      |\n",
    "# | 4         | 15701354   | Boni     | 699         | France    | Female | 39  | 1      | 0.00      | 2             | 0         | 0              | 93826.63        | 0      |\n",
    "# | 5         | 15737888   | Mitchell | 850         | Spain     | Female | 43  | 2      | 125510.82 | 1             | 1         | 1              | 79084.10        | 0      |\n",
    "\n",
    "# ✅ Drop irrelevant columns\n",
    "data.drop(columns=[\"RowNumber\", \"CustomerId\", \"Surname\"], inplace=True)\n",
    "data.head()\n",
    "# | Index | CreditScore | Geography | Gender | Age | Tenure |   Balance   | NumOfProducts | HasCrCard | IsActiveMember | EstimatedSalary | Exited |\n",
    "# |-------|-------------|-----------|--------|-----|--------|-------------|----------------|-----------|----------------|------------------|--------|\n",
    "# |   0   |     619     |  France   | Female |  42 |    2   |    0.00     |       1        |     1     |        1       |    101348.88     |   1    |\n",
    "# |   1   |     608     |  Spain    | Female |  41 |    1   |  83807.86   |       1        |     0     |        1       |    112542.58     |   0    |\n",
    "# |   2   |     502     |  France   | Female |  42 |    8   | 159660.80   |       3        |     1     |        0       |    113931.57     |   1    |\n",
    "# |   3   |     699     |  France   | Female |  39 |    1   |    0.00     |       2        |     0     |        0       |     93826.63     |   0    |\n",
    "# |   4   |     850     |  Spain    | Female |  43 |    2   | 125510.82   |       1        |     1     |        1       |     79084.10     |   0    |\n",
    "\n",
    "# ✅ 6. Label Encoding on Gender column\n",
    "# For Categorical data(Gender,Geography ), we will be using some type of encoding \n",
    "labelEncoder_gender = LabelEncoder()\n",
    "data['Gender'] = labelEncoder_gender.fit_transform(data['Gender']) \n",
    "data.head()\n",
    "# | Index | CreditScore | Geography | Gender | Age | Tenure | Balance   | NumOfProducts | HasCrCard | IsActiveMember | EstimatedSalary | Exited |\n",
    "# | ----- | ----------- | --------- | ------ | --- | ------ | --------- | ------------- | --------- | -------------- | --------------- | ------ |\n",
    "# | 0     | 619         | France    | 0      | 42  | 2      | 0.00      | 1             | 1         | 1              | 101348.88       | 1      |\n",
    "# | 1     | 608         | Spain     | 0      | 41  | 1      | 83807.86  | 1             | 0         | 1              | 112542.58       | 0      |\n",
    "# | 2     | 502         | France    | 0      | 42  | 8      | 159660.80 | 3             | 1         | 0              | 113931.57       | 1      |\n",
    "# | 3     | 699         | France    | 0      | 39  | 1      | 0.00      | 2             | 0         | 0              | 93826.63        | 0      |\n",
    "# | 4     | 850         | Spain     | 0      | 43  | 2      | 125510.82 | 1             | 1         | 1              | 79084.10        | 0      |\n",
    "# ✅ 7. Problem with Geography column\n",
    "# For Geography, we will be using one hot encoding because \n",
    "# if we 0,1,2 to represent the different geographies, ML think it is ranking so we use one hot encoding  \n",
    "# which give different values to different geographies\n",
    "\n",
    "# ✅ 8. Use One-Hot Encoding for Geography\n",
    "one_hot_encoder_geo = OneHotEncoder()\n",
    "geo_encoded = one_hot_encoder_geo.fit_transform(data[['Geography']])\n",
    "geo_encoded\n",
    "# <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
    "# \twith 10000 stored elements and shape (10000, 3)>\n",
    "# Convert sparse matrix to dense array\n",
    "geo_encoded_array = geo_encoded.toarray()\n",
    "\n",
    "# Create a DataFrame from the array with proper column names\n",
    "geo_encoded_df = pd.DataFrame(\n",
    "    geo_encoded_array,\n",
    "    columns=one_hot_encoder_geo.get_feature_names_out(['Geography'])\n",
    ")\n",
    "\n",
    "# ✅ 9. Drop old Geography column and add new encoded columns\n",
    "# Match index with original data\n",
    "geo_encoded_df.index = data.index\n",
    "\n",
    "# Drop original 'Geography' column\n",
    "data.drop(columns=['Geography'], inplace=True)\n",
    "\n",
    "# Concatenate one-hot encoded columns with original data\n",
    "data = pd.concat([data, geo_encoded_df], axis=1)\n",
    "\n",
    "# View updated dataset\n",
    "data.head()\n",
    "# | Index | CreditScore | Gender | Age | Tenure | Balance   | NumOfProducts | HasCrCard | IsActiveMember | EstimatedSalary | Exited | Geography_France | Geography_Germany | Geography_Spain |\n",
    "# | ----- | ----------- | ------ | --- | ------ | --------- | ------------- | --------- | -------------- | --------------- | ------ | ----------------- | ------------------ | ---------------- |\n",
    "# | 0     | 619         | 0      | 42  | 2      | 0.00      | 1             | 1         | 1              | 101348.88       | 1      | 1.0               | 0.0                | 0.0              |\n",
    "# | 1     | 608         | 0      | 41  | 1      | 83807.86  | 1             | 0         | 1              | 112542.58       | 0      | 0.0               | 0.0                | 1.0              |\n",
    "# | 2     | 502         | 0      | 42  | 8      | 159660.80 | 3             | 1         | 0              | 113931.57       | 1      | 1.0               | 0.0                | 0.0              |\n",
    "# | 3     | 699         | 0      | 39  | 1      | 0.00      | 2             | 0         | 0              | 93826.63        | 0      | 1.0               | 0.0                | 0.0              |\n",
    "# | 4     | 850         | 0      | 43  | 2      | 125510.82 | 1             | 1         | 1              | 79084.10        | 0      | 0.0               | 0.0                | 1.0              |\n",
    "\n",
    "\n",
    "# ✅ 10. Save the encoders using Pickle\n",
    "# Save LabelEncoder for Gender\n",
    "with open('label_encoder_gender.pkl', 'wb') as file:\n",
    "    pickle.dump(labelEncoder_gender, file)\n",
    "\n",
    "# Save OneHotEncoder for Geography\n",
    "with open('one_hot_encoder_geo.pkl', 'wb') as file:\n",
    "    pickle.dump(one_hot_encoder_geo, file)\n",
    "\n",
    "# ✅ 11. Split dataset into Independent & Dependent Features\n",
    "# Independent features\n",
    "X = data.drop(columns=['Exited'])\n",
    "\n",
    "# Dependent feature\n",
    "y = data['Exited']\n",
    "\n",
    "# Check the shapes\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# X shape: (10000, 12)\n",
    "# y shape: (10000,)\n",
    "\n",
    "# ✅ 12. Split into Train and Test sets\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "# ✅ 13. Apply Feature Scaling using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# • Scaling makes all feature values similar\n",
    "# → Useful especially for algorithms like Neural Networks\n",
    "\n",
    "# ✅ 14. Save the scaler to a file\n",
    "# Save the scaler to a file\n",
    "with open('standard_scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd5672",
   "metadata": {},
   "source": [
    "### ANN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daad9a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m832\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,945</span> (11.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,945\u001b[0m (11.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,945</span> (11.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,945\u001b[0m (11.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8000 - loss: 0.4569 - val_accuracy: 0.8290 - val_loss: 0.3957\n",
      "Epoch 2/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8453 - loss: 0.3874 - val_accuracy: 0.8570 - val_loss: 0.3537\n",
      "Epoch 3/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8544 - loss: 0.3563 - val_accuracy: 0.8630 - val_loss: 0.3417\n",
      "Epoch 4/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8595 - loss: 0.3451 - val_accuracy: 0.8600 - val_loss: 0.3409\n",
      "Epoch 5/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8614 - loss: 0.3378 - val_accuracy: 0.8595 - val_loss: 0.3442\n",
      "Epoch 6/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8612 - loss: 0.3340 - val_accuracy: 0.8580 - val_loss: 0.3439\n",
      "Epoch 7/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8665 - loss: 0.3301 - val_accuracy: 0.8670 - val_loss: 0.3379\n",
      "Epoch 8/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8645 - loss: 0.3286 - val_accuracy: 0.8580 - val_loss: 0.3419\n",
      "Epoch 9/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8650 - loss: 0.3256 - val_accuracy: 0.8635 - val_loss: 0.3373\n",
      "Epoch 10/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8655 - loss: 0.3237 - val_accuracy: 0.8640 - val_loss: 0.3356\n",
      "Epoch 11/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8673 - loss: 0.3223 - val_accuracy: 0.8625 - val_loss: 0.3365\n",
      "Epoch 12/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8677 - loss: 0.3201 - val_accuracy: 0.8625 - val_loss: 0.3377\n",
      "Epoch 13/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8681 - loss: 0.3188 - val_accuracy: 0.8630 - val_loss: 0.3415\n",
      "Epoch 14/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8695 - loss: 0.3169 - val_accuracy: 0.8590 - val_loss: 0.3374\n",
      "Epoch 15/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8684 - loss: 0.3159 - val_accuracy: 0.8610 - val_loss: 0.3372\n",
      "Epoch 16/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8700 - loss: 0.3137 - val_accuracy: 0.8625 - val_loss: 0.3391\n",
      "Epoch 17/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8699 - loss: 0.3136 - val_accuracy: 0.8645 - val_loss: 0.3370\n",
      "Epoch 18/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8712 - loss: 0.3109 - val_accuracy: 0.8570 - val_loss: 0.3461\n",
      "Epoch 19/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8701 - loss: 0.3096 - val_accuracy: 0.8650 - val_loss: 0.3390\n",
      "Epoch 20/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8708 - loss: 0.3084 - val_accuracy: 0.8580 - val_loss: 0.3458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__) # 2.19.0\n",
    "\n",
    "# 🔹 Step 1: Import Required Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import datetime\n",
    "# •\t📦 tensorflow: Main deep learning framework\n",
    "# •\t🏗️ Sequential: To stack layers sequentially\n",
    "# •\t🧱 Dense: Fully connected layer\n",
    "# •\t🎛️ EarlyStopping, TensorBoard: For training control and visualization\n",
    "# •\t🕒 datetime: To timestamp log folders\n",
    "\n",
    "# 🔹 Step 2: Build and Compile the Model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],))) # HL1 connected with input layer\n",
    "model.add(Dense(32, activation='relu')) # HL2 we don’t need to give input_shape because in Sequential it already know it connected \n",
    "model.add(Dense(1, activation='sigmoid'))#only 1 output layer with activation function is sigmoid \n",
    "\n",
    "# 2.19.0\n",
    "# c:\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
    "#   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
    "\n",
    "model.summary()\n",
    "# Model: \"sequential_3\"\n",
    "#  Total params: 2,945 (11.50 KB)\n",
    "#  Trainable params: 2,945 (11.50 KB)\n",
    "#  Non-trainable params: 0 (0.00 B)\n",
    "\n",
    "# ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
    "# ┃ Layer (type)                    ┃ Output Shape           ┃ Param #       ┃\n",
    "# ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
    "# │ dense_3 (Dense)                 │ (None, 64)             │ 832           │\n",
    "# ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "# │ dense_4 (Dense)                 │ (None, 32)             │ 2,080         │\n",
    "# ├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "# │ dense_5 (Dense)                 │ (None, 1)              │ 33            │\n",
    "# └─────────────────────────────────┴────────────────────────┴───────────────┘\n",
    "\n",
    "#compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"binary_crossentropy\", # loss function is binary_crossentropy which is used for binary classification\n",
    "    metrics=['accuracy'] # accuracy is a metric to measure how good the model is\n",
    ")\n",
    "\n",
    "# 🔹 Step 3: Create Log Directory for TensorBoard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# 🗂️ This creates a log folder like: logs/fit/20250731-142530\n",
    "# •\tHelps you differentiate runs\n",
    "# •\tKeeps logs neatly organized\n",
    "\n",
    "# 🔹 Step 4: Create TensorBoard Callback\n",
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1\n",
    ")\n",
    "# •\t🧪 histogram_freq=1: Logs weight histograms every epoch\n",
    "# •\t📊 Enables real-time visualization in TensorBoard\n",
    "# •\t📊 tensorboard_callback is used to visualize the model\n",
    "\n",
    "# 🔹 Step 5: Set Up EarlyStopping\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',        # Watch validation loss\n",
    "    patience=10,                # Wait 10 epochs before stopping\n",
    "    restore_best_weights=True # Reload best model\n",
    ")\n",
    "# 🧠 Why use it?\n",
    "# •\tSaves training time\n",
    "# •\tAvoids overfitting\n",
    "# •\tKeeps best version of model\n",
    "\n",
    "# 🔹 Step 6: Train the Model with Callbacks\n",
    "# X_train,X_test,y_train,y_test\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100, # Number of epochs is used to train the model\n",
    "    callbacks=[tensorboard_callback, early_stop]\n",
    ")\n",
    "# ✅ Benefits:\n",
    "# •\tTraining will stop early if no improvement\n",
    "# •\tAll logs will be stored and visualized\n",
    "\n",
    "# Epoch 1/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8136 - loss: 0.4391 - val_accuracy: 0.8395 - val_loss: 0.3815\n",
    "# Epoch 2/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8453 - loss: 0.3720 - val_accuracy: 0.8535 - val_loss: 0.3527\n",
    "# Epoch 3/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8565 - loss: 0.3495 - val_accuracy: 0.8535 - val_loss: 0.3459\n",
    "# Epoch 4/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8594 - loss: 0.3421 - val_accuracy: 0.8625 - val_loss: 0.3460\n",
    "# Epoch 5/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8614 - loss: 0.3367 - val_accuracy: 0.8600 - val_loss: 0.3457\n",
    "# Epoch 6/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8618 - loss: 0.3346 - val_accuracy: 0.8620 - val_loss: 0.3426\n",
    "# Epoch 7/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8640 - loss: 0.3305 - val_accuracy: 0.8605 - val_loss: 0.3433\n",
    "# Epoch 8/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8629 - loss: 0.3294 - val_accuracy: 0.8580 - val_loss: 0.3429\n",
    "# Epoch 9/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8618 - loss: 0.3281 - val_accuracy: 0.8615 - val_loss: 0.3464\n",
    "# Epoch 10/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8652 - loss: 0.3247 - val_accuracy: 0.8585 - val_loss: 0.3419\n",
    "# Epoch 11/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8652 - loss: 0.3234 - val_accuracy: 0.8670 - val_loss: 0.3386\n",
    "# Epoch 12/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8655 - loss: 0.3229 - val_accuracy: 0.8640 - val_loss: 0.3393\n",
    "# Epoch 13/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8639 - loss: 0.3216 - val_accuracy: 0.8620 - val_loss: 0.3428\n",
    "# Epoch 14/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8655 - loss: 0.3184 - val_accuracy: 0.8555 - val_loss: 0.3436\n",
    "# Epoch 15/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8681 - loss: 0.3166 - val_accuracy: 0.8595 - val_loss: 0.3382\n",
    "# Epoch 16/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8684 - loss: 0.3157 - val_accuracy: 0.8590 - val_loss: 0.3403\n",
    "# Epoch 17/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8670 - loss: 0.3148 - val_accuracy: 0.8585 - val_loss: 0.3382\n",
    "# Epoch 18/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8677 - loss: 0.3137 - val_accuracy: 0.8590 - val_loss: 0.3402\n",
    "# Epoch 19/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8677 - loss: 0.3128 - val_accuracy: 0.8545 - val_loss: 0.3460\n",
    "# Epoch 20/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8689 - loss: 0.3116 - val_accuracy: 0.8605 - val_loss: 0.3454\n",
    "# Epoch 21/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8687 - loss: 0.3101 - val_accuracy: 0.8575 - val_loss: 0.3420\n",
    "# Epoch 22/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8686 - loss: 0.3089 - val_accuracy: 0.8565 - val_loss: 0.3453\n",
    "# Epoch 23/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8702 - loss: 0.3089 - val_accuracy: 0.8630 - val_loss: 0.3395\n",
    "# Epoch 24/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8710 - loss: 0.3080 - val_accuracy: 0.8590 - val_loss: 0.3445\n",
    "# Epoch 25/100\n",
    "# \u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8721 - loss: 0.3063 - val_accuracy: 0.8555 - val_loss: 0.3442\n",
    "\n",
    "# 🔹 Step 7: Save the Trained Model\n",
    "model.save('model.h5')\n",
    "# 💾 Saves the entire model (architecture + weights)\n",
    "# Can be loaded later using:\n",
    "model = tf.keras.models.load_model('model.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e6ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 15716), started 0:02:42 ago. (Use '!kill 15716' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-54fe28b72b718f32\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-54fe28b72b718f32\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 🔹 Step 8: Load TensorBoard in Notebook\n",
    "%load_ext tensorboard \n",
    "%tensorboard --logdir logs/fit\n",
    "# # 📊 This starts TensorBoard in your Jupyter notebook\n",
    "# # 🛠️ If using a specific run:\n",
    "# %tensorboard --logdir logs/fit/20250731-174609\n",
    "\n",
    "# 🔹 Step 9: Understanding TensorBoard Graphs\n",
    "# 🧠 Key Charts You’ll See:\n",
    "# 🔍 Metric\t         📈 Meaning\n",
    "# epoch_accuracy\tModel's accuracy per epoch\n",
    "# epoch_loss\t    Training loss per epoch\n",
    "# val_accuracy\t    Validation accuracy (generalization check)\n",
    "# val_loss\t        Validation loss (used by EarlyStopping)\n",
    "# Histograms\t    Weight/bias distributions layer-wise\n",
    "#biases\t            Weights of the dense layer\n",
    "# epochslearningrate\tLearning rate per epoch\n",
    "# ✅ Good sign:\n",
    "# •\t📉 Loss goes down\n",
    "# •\t📈 Accuracy goes up\n",
    "\n",
    "\n",
    "# 🧠 What is an Epoch in Machine Learning (especially Deep Learning)?\n",
    "# An epoch is one complete pass through all the training data during model training.\n",
    "\n",
    "# ✅ Example to Understand:\n",
    "# Suppose you have:\n",
    "\n",
    "# x_train = 1000 training samples\n",
    "# batch_size = 100\n",
    "# ➡️ One epoch = model sees all 1000 samples once, usually in 10 batches of 100.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
